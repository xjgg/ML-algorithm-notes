1. 机器学习概念
1.1 有监督学习和无监督学习：根据训练数据是否拥有标记信息，学习任务可大致划分为两大类：监督学习和无监督学习。分类和回归是前者的代表，聚类是后者的代表。
1.2 泛化能力：学得模型适用于新样本的能力，称为“泛化”能力。具有强泛化能力的模型能很好地适用于整个样本空间。
1.3 过拟合欠拟合(方差和偏差以及各自解决办法)：我们希望在新样本上能表现得很好的学习器。为了达到这个目的，应该从训练样本中尽可能学出适用于所有潜在样本的“普遍规律”，这样才能在遇到新样本时做出正确的判别。然而，当学习器把训练样本学得“太好”了的时候，很可能已经把训练样本本身的一些特点当做了所有潜在样本都会具有的一般性质，这样就会导致泛化能力下降。这种现象在机器学习中称为“过拟合”。与过拟合对应的是“欠拟合”，这是指对训练样本的一般性质尚未学好。通常我们可以通过实验测试来对学习器的泛化误差进行评估进而做出选择。为此需使用一个测试集来测试学习器对新样本的判别能力，然后以测试集上的测试误差作为泛化误差的近似。
1.4 交叉验证：交叉验证法先将数据集D划分为k个大小相似的互斥子集，每个子集都尽可能保持数据分布的一致性，即从D中通过分层采样得到。然后每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集；这样就可以获得k组训练/测试集，从而可进行k次训练和测试，最终返回的是这k个测试结果的均值。

2. 线性回归的原理
线型回归试图学得一个线型模型以尽可能准确地预测实值输出标记。即试图学得f(xi) = wxi + b

3. 线性回归损失函数、代价函数、目标函数
建模误差的平方和

4. 优化方法(梯度下降法、牛顿法、拟牛顿法等)
梯度下降：用来求函数最小值的算法。开始时随机选择一个参数的组合，计算代价函数，然后寻找下一个能让代价函数值下降最多的参数组合。持续这么做直到找到一个局部最小值。
牛顿法和拟牛顿法不懂
5、线性回归的评估指标
均方误差是回归任务中最常用的性能度量，基于均方误差最小化进行模型求解的方法称为“最小二乘法”。在线型回归中，最小二乘法就是试图找到一条直线，使得所有样本到直线上的欧氏距离之和最小。
6、sklearn参数详解
from sklearn.linear_model import LinearRegression
sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)
fit_intercept=True：默认计算截距
normalize=False：默认不进行归一化
copy_X=True：默认数据复制
n_jobs=None：加速运算
